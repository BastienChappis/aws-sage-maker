{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will take the .flow files that define the transformations to the raw data. and apply them using a SageMaker Processing job that will apply those transformations to the raw data deposited in the S3 bucket as .csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. SETUP\n",
    "## 0.1 Install required and/or update third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -Uq pip\n",
    "!python -m pip install -q awswrangler==2.2.0 imbalanced-learn==0.7.0 sagemaker==2.41.0 boto3==1.17.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Loading stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don’t see anything printed then it’s probably the first time you are running the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket             -> 'sagemaker-eu-west-1-688316159674'\n",
      "prefix             -> 'fraud-detect-demo'\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import string\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Getting started: Creating Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully run this notebook you will need to create some AWS resources. First, an S3 bucket will be created to store all the data for this tutorial. Once created, you will then need to create an AWS Glue role using the IAM console then attach a policy to the S3 bucket to allow FeatureStore access to this notebook. If you’ve already run this notebook and are picking up where you left off, then running the cells below should pick up the resources you already created without creating any additional resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In a separate brower tab go to the IAM section of the AWS Console\n",
    "2. Navigate to the Roles section and select the execution role you’re using for your SageMaker Studio user\n",
    "    - If you’re not sure what role you’re using, run the cell below to print it out\n",
    "3.Attach the AmazonSageMakerFeatureStoreAccess policy to this role. Once attached, the changes take effect immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Role: AmazonSageMaker-ExecutionRole-FiftyFive\n"
     ]
    }
   ],
   "source": [
    "print(\"SageMaker Role:\", sagemaker.get_execution_role().split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AWS Region: eu-west-1\n"
     ]
    }
   ],
   "source": [
    "# You can change this to a region of your choice\n",
    "import sagemaker\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(region_name=region)\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: if you are not running this notebook from SageMaker Studio or SageMaker Classic Notebooks you will need to instanatiate\n",
    "the sagemaker_execution_role_name with an AWS role that has SageMakerFullAccess and SageMakerFeatureStoreFullAccess\n",
    "\"\"\"\n",
    "sagemaker_execution_role_name = \"AmazonSageMaker-ExecutionRole-20210107T234882\"\n",
    "try:\n",
    "    sagemaker_role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    sagemaker_role = iam.get_role(RoleName=sagemaker_execution_role_name)[\"Role\"][\"Arn\"]\n",
    "    print(f\"\\n instantiating sagemaker_role with supplied role name : {sagemaker_role}\")\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Create a directory in the SageMaker default bucket for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bucket' not in locals():\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "    prefix = 'fraud-detect-demo'\n",
    "    %store bucket\n",
    "    %store prefix\n",
    "    print(f'Creating bucket: {bucket}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use your own S3 bucket that’s already existing, uncomment and utilize the following example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntry:\\n    s3_client.create_bucket(Bucket=bucket, ACL='private', CreateBucketConfiguration={'LocationConstraint': region})\\n    print('Create S3 bucket: SUCCESS')\\n\\nexcept Exception as e:\\n    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\\n        print(f'Using existing bucket: {bucket}/{prefix}')\\n    else:\\n        raise(e)\\n\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "try:\n",
    "    s3_client.create_bucket(Bucket=bucket, ACL='private', CreateBucketConfiguration={'LocationConstraint': region})\n",
    "    print('Create S3 bucket: SUCCESS')\n",
    "\n",
    "except Exception as e:\n",
    "    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "        print(f'Using existing bucket: {bucket}/{prefix}')\n",
    "    else:\n",
    "        raise(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======> Tons of output_paths\n",
    "traing_job_output_path = f\"s3://{bucket}/{prefix}/training_jobs\"\n",
    "bias_report_1_output_path = f\"s3://{bucket}/{prefix}/clarify-bias-1\"\n",
    "bias_report_2_output_path = f\"s3://{bucket}/{prefix}/clarify-bias-2\"\n",
    "explainability_output_path = f\"s3://{bucket}/{prefix}/clarify-explainability\"\n",
    "\n",
    "train_data_uri = f\"s3://{bucket}/{prefix}/data/train/train.csv\"\n",
    "test_data_uri = f\"s3://{bucket}/{prefix}/data/test/test.csv\"\n",
    "\n",
    "# =======> variables used for parameterizing the notebook run\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "claify_instance_count = 1\n",
    "clairfy_instance_type = \"ml.c5.xlarge\"\n",
    "\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.c5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.6 Upload raw data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"data/claims.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/claims.csv\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"data/customers.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/customers.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 DATA WRANGLING\n",
    "## 1.1 Update attributes within the .flow file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataWrangler will generate a .flow file. It contains a reference to an S3 bucket used during the Wrangling. This may be different from the one you have as a default in this notebook eg if the Wrangling was done by someone else, you will probably not have access to their bucket and you now need to point to your own S3 bucket so you can actually load the .flow file into Wrangler or access the data.\n",
    "\n",
    "Running the cell below wil genereate the data wrangler flow file and export the data to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Save to S3 with a SageMaker Processing Job\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Quick Start </strong>\n",
    "To save your processed data to S3, select the Run menu above and click <strong>Run all cells</strong>. \n",
    "<strong><a style=\"color: #0397a7 \" href=\"#Job-Status-&-S3-Output-Location\">\n",
    "    <u>View the status of the export job and the output S3 location</u></a>.\n",
    "</strong>\n",
    "</div>\n",
    "\n",
    "\n",
    "This notebook executes your Data Wrangler Flow `customers.flow` on the entire dataset using a SageMaker \n",
    "Processing Job and will save the processed data to S3.\n",
    "\n",
    "This notebook saves data from the step `Cast Single Data Type` from `Source: Customers.Csv`. To save from a different step, go to Data Wrangler \n",
    "to select a new step to export. \n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Inputs and Outputs](#Inputs-and-Outputs)\n",
    "1. [Run Processing Job](#Run-Processing-Job)\n",
    "   1. [Job Configurations](#Job-Configurations)\n",
    "   1. [Create Processing Job](#Create-Processing-Job)\n",
    "   1. [Job Status & S3 Output Location](#Job-Status-&-S3-Output-Location)\n",
    "1. [Optional Next Steps](#(Optional)Next-Steps)\n",
    "    1. [Load Processed Data into Pandas](#(Optional)-Load-Processed-Data-into-Pandas)\n",
    "    1. [Train a model with SageMaker](#(Optional)Train-a-model-with-SageMaker)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Inputs and Outputs\n",
    "\n",
    "The below settings configure the inputs and outputs for the flow export.\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Configurable Settings </strong>\n",
    "\n",
    "In <b>Input - Source</b> you can configure the data sources that will be used as input by Data Wrangler\n",
    "\n",
    "1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n",
    "2. For all other sources, configure attributes like query_string, database in the source's \n",
    "<b>DatasetDefinition</b> object.\n",
    "\n",
    "If you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \n",
    "You should also re-execute the cells in this section if you have modified the settings in any data sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "import time\n",
    "import uuid\n",
    "import sagemaker\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Input - S3 data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the datasources\n",
    "data_sources = []\n",
    "\n",
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://sagemaker-eu-west-1-688316159674/fraud-detect-demo/data/raw/customers.csv\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/customers.csv\",\n",
    "    input_name=\"customers.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Output: S3 settings\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Configurable Settings </strong>\n",
    "\n",
    "1. <b>bucket</b>: you can configure the S3 bucket where Data Wrangler will save the output. The default bucket from \n",
    "the SageMaker notebook session is used. \n",
    "2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \n",
    "conflict with other flow exports \n",
    "3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler export storage bucket: sagemaker-eu-west-1-688316159674\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# You can configure this with your own bucket name, e.g.\n",
    "# bucket = \"my-bucket\"\n",
    "bucket = sess.default_bucket()\n",
    "print(f\"Data Wrangler export storage bucket: {bucket}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the inputs required by the SageMaker Python SDK to launch a processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow S3 export result path: s3://sagemaker-eu-west-1-688316159674/export-flow-2021-02-12-54-26-claims/output\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "output_name = \"a38c7be6-7fdb-4445-95b9-065b4a88eab5.default\"\n",
    "\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "print(f\"Flow S3 export result path: {s3_output_path}\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_output_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Upload Flow to S3\n",
    "\n",
    "To use the Data Wrangler as an input to the processing job,  first upload your flow file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flow file from current notebook working directory: /root/full-ml-process\n",
      "Data Wrangler flow customers.flow uploaded to s3://sagemaker-eu-west-1-688316159674/data_wrangler_flows/customers.flow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"customers.flow\"\n",
    "\n",
    "# unique flow export ID\n",
    "flow_export_id = f\"{time.strftime('%Y-%d-%H-%M-%S', time.gmtime())}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}-{flow_file_name}\".replace(\".csv\", \"\")\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"data_wrangler_flows/{flow_file_name}\")\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/data_wrangler_flows/{flow_file_name}\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Wrangler Flow is also provided to the Processing Job as an input source which we configure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: customers.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Run Processing Job \n",
    "### 1.4.1 Job Configurations\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Configurable Settings </strong>\n",
    "\n",
    "You can configure the following settings for Processing Jobs. If you change any configurations you will \n",
    "need to re-execute this and all cells below it by selecting the Run menu above and click \n",
    "<b>Run Selected Cells and All Below</b>\n",
    "\n",
    "1. IAM role for executing the processing job. \n",
    "2. A unique name of the processing job. Give a unique name every time you re-execute processing jobs\n",
    "3. Data Wrangler Container URL.\n",
    "4. Instance count, instance type and storage volume size in GB.\n",
    "5. Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "6. Network Isolation settings\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = \"245179582081.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "# Pinned Data Wrangler Container URL. \n",
    "container_uri_pinned = \"245179582081.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-data-wrangler-container:1.6.2\"\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Network Isolation mode; default is off\n",
    "enable_network_isolation = False\n",
    "\n",
    "# Output configuration used as processing job container arguments \n",
    "output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": output_content_type\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Create Processing Job\n",
    "\n",
    "To launch a Processing Job, you will use the SageMaker Python SDK to create a Processor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-wrangler-flow-processing-2021-03-07-34-15\n",
      "Inputs:  [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-688316159674/data_wrangler_flows/customers.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'customers.csv', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-688316159674/fraud-detect-demo/data/raw/customers.csv', 'LocalPath': '/opt/ml/processing/customers.csv', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'a38c7be6-7fdb-4445-95b9-065b4a88eab5.default', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-688316159674/export-flow-2021-02-12-54-26-claims/output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=NetworkConfig(enable_network_isolation=enable_network_isolation),\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Start Job\n",
    "processor.run(\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output],\n",
    "    arguments=[f\"--output-config '{json.dumps(output_config)}'\"],\n",
    "    wait=False,\n",
    "    logs=False,\n",
    "    job_name=processing_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Job Status & S3 Output Location\n",
    "\n",
    "Below you wait for processing job to finish. If it finishes successfully, the raw parameters used by the \n",
    "Processing Job will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job results are saved to S3 path: s3://sagemaker-eu-west-1-688316159674/export-flow-2021-02-12-54-26-claims/output/data-wrangler-flow-processing-2021-03-07-34-15\n",
      "......................................"
     ]
    }
   ],
   "source": [
    "s3_job_results_path = f\"s3://{bucket}/{s3_output_prefix}/{processing_job_name}\"\n",
    "print(f\"Job results are saved to S3 path: {s3_job_results_path}\")\n",
    "\n",
    "job_result = sess.wait_for_processing_job(processing_job_name)\n",
    "job_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claims data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources=[]\n",
    "\n",
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://sagemaker-eu-west-1-688316159674/fraud-detect-demo/data/raw/claims.csv\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/claims.csv\",\n",
    "    input_name=\"claims.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# You can configure this with your own bucket name, e.g.\n",
    "# bucket = \"my-bucket\"\n",
    "bucket = sess.default_bucket()\n",
    "print(f\"Data Wrangler export storage bucket: {bucket}\")\n",
    "\n",
    "# unique flow export ID\n",
    "flow_export_id = f\"{time.strftime('%Y-%d-%H-%M-%S', time.gmtime())}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}-{flow_file_name}\".replace(\".csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "output_name = \"9ff4c2a4-e1fe-4a66-819f-f7d8df7a5fed.default\"\n",
    "\n",
    "\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "print(f\"Flow S3 export result path: {s3_output_path}\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_output_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"claims.flow\"\n",
    "\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"data_wrangler_flows/{flow_file_name}\")\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/data_wrangler_flows/{flow_file_name}\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: claims.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = \"245179582081.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "# Pinned Data Wrangler Container URL. \n",
    "container_uri_pinned = \"245179582081.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-data-wrangler-container:1.6.2\"\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Network Isolation mode; default is off\n",
    "enable_network_isolation = False\n",
    "\n",
    "# Output configuration used as processing job container arguments \n",
    "output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": output_content_type\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=NetworkConfig(enable_network_isolation=enable_network_isolation),\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Start Job\n",
    "processor.run(\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output],\n",
    "    arguments=[f\"--output-config '{json.dumps(output_config)}'\"],\n",
    "    wait=False,\n",
    "    logs=False,\n",
    "    job_name=processing_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_job_results_path = f\"s3://{bucket}/{s3_output_prefix}/{processing_job_name}\"\n",
    "print(f\"Job results are saved to S3 path: {s3_job_results_path}\")\n",
    "\n",
    "job_result = sess.wait_for_processing_job(processing_job_name)\n",
    "job_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processed Data into Pandas\n",
    "\n",
    "We use the [AWS Data Wrangler library](https://github.com/awslabs/aws-data-wrangler) to load the exported \n",
    "dataset into a Pandas dataframe for a preview of first 1000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q awswrangler pandas\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>customer_education</th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>policy_liability</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>num_claims_past_year</th>\n",
       "      <th>num_insurers_past_5_years</th>\n",
       "      <th>customer_gender_male</th>\n",
       "      <th>customer_gender_female</th>\n",
       "      <th>customer_gender_unkown</th>\n",
       "      <th>customer_gender_other</th>\n",
       "      <th>policy_state_ca</th>\n",
       "      <th>policy_state_wa</th>\n",
       "      <th>policy_state_az</th>\n",
       "      <th>policy_state_or</th>\n",
       "      <th>policy_state_nv</th>\n",
       "      <th>policy_state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>165</td>\n",
       "      <td>750</td>\n",
       "      <td>2950</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>155</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>996</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>997</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>998</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>999</td>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>184</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     policy_id  customer_age  customer_education  months_as_customer  \\\n",
       "0            1            54                   2                  94   \n",
       "1            2            41                   3                 165   \n",
       "2            3            57                   3                 155   \n",
       "3            4            39                   4                  80   \n",
       "4            5            39                   1                  60   \n",
       "..         ...           ...                 ...                 ...   \n",
       "995        996            41                   3                  91   \n",
       "996        997            45                   3                  60   \n",
       "997        998            41                   4                  96   \n",
       "998        999            63                   3                 184   \n",
       "999       1000            33                   1                  50   \n",
       "\n",
       "     policy_deductable  policy_annual_premium  policy_liability  auto_year  \\\n",
       "0                  750                   3000                 1       2006   \n",
       "1                  750                   2950                 0       2012   \n",
       "2                  750                   3000                 0       2017   \n",
       "3                  750                   3000                 2       2020   \n",
       "4                  750                   3000                 0       2018   \n",
       "..                 ...                    ...               ...        ...   \n",
       "995                750                   3000                 0       2016   \n",
       "996                750                   3000                 3       2019   \n",
       "997                750                   3000                 0       2019   \n",
       "998                750                   3000                 2       2019   \n",
       "999                750                   3000                 1       2013   \n",
       "\n",
       "     num_claims_past_year  num_insurers_past_5_years  customer_gender_male  \\\n",
       "0                       0                          1                     0   \n",
       "1                       0                          1                     1   \n",
       "2                       0                          1                     0   \n",
       "3                       0                          1                     0   \n",
       "4                       0                          1                     0   \n",
       "..                    ...                        ...                   ...   \n",
       "995                     0                          1                     1   \n",
       "996                     0                          1                     0   \n",
       "997                     0                          1                     1   \n",
       "998                     0                          1                     0   \n",
       "999                     0                          1                     0   \n",
       "\n",
       "     customer_gender_female  customer_gender_unkown  customer_gender_other  \\\n",
       "0                         0                       1                    0.0   \n",
       "1                         0                       0                    0.0   \n",
       "2                         1                       0                    0.0   \n",
       "3                         1                       0                    0.0   \n",
       "4                         1                       0                    0.0   \n",
       "..                      ...                     ...                    ...   \n",
       "995                       0                       0                    0.0   \n",
       "996                       1                       0                    0.0   \n",
       "997                       0                       0                    0.0   \n",
       "998                       1                       0                    0.0   \n",
       "999                       1                       0                    0.0   \n",
       "\n",
       "     policy_state_ca  policy_state_wa  policy_state_az  policy_state_or  \\\n",
       "0                0.0              1.0              0.0              0.0   \n",
       "1                1.0              0.0              0.0              0.0   \n",
       "2                1.0              0.0              0.0              0.0   \n",
       "3                0.0              0.0              1.0              0.0   \n",
       "4                1.0              0.0              0.0              0.0   \n",
       "..               ...              ...              ...              ...   \n",
       "995              1.0              0.0              0.0              0.0   \n",
       "996              0.0              1.0              0.0              0.0   \n",
       "997              1.0              0.0              0.0              0.0   \n",
       "998              0.0              1.0              0.0              0.0   \n",
       "999              1.0              0.0              0.0              0.0   \n",
       "\n",
       "     policy_state_nv  policy_state_id  \n",
       "0                0.0              0.0  \n",
       "1                0.0              0.0  \n",
       "2                0.0              0.0  \n",
       "3                0.0              0.0  \n",
       "4                0.0              0.0  \n",
       "..               ...              ...  \n",
       "995              0.0              0.0  \n",
       "996              0.0              0.0  \n",
       "997              0.0              0.0  \n",
       "998              0.0              0.0  \n",
       "999              0.0              0.0  \n",
       "\n",
       "[1000 rows x 20 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize = 1000\n",
    "\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "elif output_content_type.upper() == \"PARQUET\":\n",
    "    dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "df = next(dfs)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
